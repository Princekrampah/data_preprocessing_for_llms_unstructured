{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing The Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "Document preprocessing includes a few steps we have to follow:\n",
    "\n",
    "1. **Document context:** Text content from the document used to perform similarity search or/and keyword search in RAG applications.\n",
    "\n",
    "2. **Document elements:** These are the building blocks of a document. This are usefull in performing various tasks in a RAG systems such as filtering and document chuncking. These include:\n",
    "\n",
    "- Titles\n",
    "- List itesms\n",
    "- Tables\n",
    "- Images\n",
    "- Narrative text content\n",
    "\n",
    "3. **Element Metadata:** This includes a variety of things like:\n",
    "\n",
    "- Page number\n",
    "- File types\n",
    "- Section\n",
    "- File name\n",
    "\n",
    "\n",
    "This metadata is typically used in **Hybrid searches**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Is Data Preprocessing Hard?\n",
    "\n",
    "1. **Content Cues**\n",
    "\n",
    "    > Different documents have different cues for element types\n",
    "\n",
    "2. **Standardization Need**\n",
    "\n",
    "    > To process documents from different types, they need some form of data standardization which can be hard to implement.\n",
    "\n",
    "\n",
    "3. **Data Extraction Variability**\n",
    "\n",
    "    > Different documents have different ways or forms used to extract context from them. Each document format needs a different way to extract context from them.\n",
    "\n",
    "\n",
    "4. **Metadata Insight**\n",
    "\n",
    "    > Extracting metadata requires a deeper understanding of the document structure. Articles do not have pages, but PDF documents do etcetera.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why We Normalize Diverse Document Types\n",
    "\n",
    "1. **Documents come in different formats**\n",
    "\n",
    "    > When building LLM applications, the least you want to worry about is where and what format the data came from. For this very reason, it only makes sense to normalize your diverse documents.\n",
    "\n",
    "2. **Common Format**\n",
    "\n",
    "    > The first step in document preprocessing is to firt make sure the raw documents are converted into a common format that is able to identify common document elements like titles, paragraphs narative texts etcetera.\n",
    "\n",
    "\n",
    "**Normalization Benefits**\n",
    "\n",
    "1. If allows documents to be parsed down in the same way regardless of the format of origin. This allows as not to create separate parsers for each document type/format.\n",
    "\n",
    "2. Helps reduce preprocessing costs. The most expensive and time consuming part of document preprocessing is the dat extraction part. Having data that is normalized means downstream tasks have it easier on operating with normalized texts.\n",
    "\n",
    "\n",
    "### Document Serialization\n",
    "\n",
    "This is the second step right after document normalization. This allows for us to use to results of document preprocessing over and over gaint without having to start from scratch. We can serialize our data as JSON format. There are other formats, but JSON is commonly used as JSON is supported across many programming languages. JSON can also be used for streaming use cases, standard HTTP response, common and well understood structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! poetry add unstructured python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "import json\n",
    "\n",
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.partition.pptx import partition_pptx\n",
    "from unstructured.staging.base import dict_to_elements, elements_to_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse HTML Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Reading document from string ...\n",
      "INFO: Reading document ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/prince/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/prince/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "filename = \"./example_datasets/medium_blog_post.html\"\n",
    "elements = partition_html(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"690e0c655cefd034544abc23a00861d7\",\n",
      "    \"text\": \"Share\",\n",
      "    \"metadata\": {\n",
      "      \"category_depth\": 0,\n",
      "      \"last_modified\": \"2024-06-02T22:52:40\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"file_directory\": \"./example_datasets\",\n",
      "      \"filename\": \"medium_blog_post.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"2e8f75631dafe3e00405ccd3ffb99403\",\n",
      "    \"text\": \"In the vast digital universe, data is the\\n                                lifeblood that drives decision-making and\\n                                innovation. But not all data is created equal.\\n                                Unstructured data in images and documents often\\n                                hold a wealth of information that can be\\n                                challenging to extract and analyze.\",\n",
      "    \"metadata\": {\n",
      "      \"last_modified\": \"2024-06-02T22:52:40\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"690e0c655cefd034544abc23a00861d7\",\n",
      "      \"file_directory\": \"./example_datasets\",\n",
      "      \"filename\": \"medium_blog_post.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"350cecfa0bd6fafe1ac30cdd3bed160f\",\n",
      "    \"text\": \"Enter\\n                                Unstructured.io, a powerful tool to extract and efficiently\\n                                transform structured data. With sixteen and\\n                                counting pre-built connectors, the API can\\n                                easily integrate with various data sources,\\n                                including AWS S3, GitHub, Google Cloud Storage,\\n                                and more.\",\n",
      "    \"metadata\": {\n",
      "      \"last_modified\": \"2024-06-02T22:52:40\",\n",
      "      \"link_texts\": [\n",
      "        \"Unstructured.io\"\n",
      "      ],\n",
      "      \"link_urls\": [\n",
      "        \"https://www.unstructured.io/\"\n",
      "      ],\n",
      "      \"link_start_indexes\": [\n",
      "        38\n",
      "      ],\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"690e0c655cefd034544abc23a00861d7\",\n",
      "      \"file_directory\": \"./example_datasets\",\n",
      "      \"filename\": \"medium_blog_post.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"dbe846135e1a9bdff22a7a8b804c6061\",\n",
      "    \"text\": \"In this guide, we\\u2019ll cover the advantages of\\n                                using the Unstructured API and Connector module,\\n                                walk you through a step-by-step process of using\\n                                it with the S3 Connector as an example, and show\\n                                you how to be a part of the Unstructured\\n                                community.\",\n",
      "    \"metadata\": {\n",
      "      \"last_modified\": \"2024-06-02T22:52:40\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"parent_id\": \"690e0c655cefd034544abc23a00861d7\",\n",
      "      \"file_directory\": \"./example_datasets\",\n",
      "      \"filename\": \"medium_blog_post.html\",\n",
      "      \"filetype\": \"text/html\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "element_dict = [el.to_dict() for el in elements]\n",
    "example_output = json.dumps(element_dict[11:15], indent=2)\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prince/Desktop/content/courses/preprocessing_unstructured_data_for_llms/.venv/lib/python3.10/site-packages/IPython/core/display.py:618: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JSON(example_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse MS PPTX Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./example_datasets/msft_openai.pptx\"\n",
    "elements = partition_pptx(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'Title', 'element_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'text': 'ChatGPT', 'metadata': {'category_depth': 1, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': '34a50527166e6765aa3e40778b5764e1', 'text': 'Chat-GPT: AI Chatbot, developed by OpenAI,\\xa0trained to perform conversational tasks and\\xa0creative tasks', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': '631df69dff044f977d66d71c5cbdab83', 'text': 'Backed by GPT-3.5 model (gpt-35-turbo), GPT-4\\xa0models', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': '6ac7cc52b0b2842ce7803bb176add0fb', 'text': 'Trained over 175 billion machine learning parameters', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': '01133c5465c85564ab1e39568d8b51f5', 'text': 'Conversation-in and message-out\\xa0', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': '1d495819227b92f341fb4b58d723a497', 'text': 'Note: Chat Completion API for GPT-4 models', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}, {'type': 'ListItem', 'element_id': 'e450241caa0f39c30939a474bcff06ac', 'text': 'GPT-4 is multimodal (e.g., images + text)', 'metadata': {'category_depth': 0, 'file_directory': './example_datasets', 'filename': 'msft_openai.pptx', 'last_modified': '2024-06-02T23:00:54', 'page_number': 1, 'languages': ['eng'], 'parent_id': 'e53cb06805f45fa23fb6d77966c5ec63', 'filetype': 'application/vnd.openxmlformats-officedocument.presentationml.presentation'}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_dict = [el.to_dict() for el in elements]\n",
    "print(element_dict[:])\n",
    "JSON(json.dumps(element_dict[:], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process PDF documents takes abit of resources and hardware requirments. So Unstructured provides an API we can use instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! poetry add python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = UnstructuredClient(\n",
    "    api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"Title\",\n",
      "    \"element_id\": \"826446fa7830f0352c88808f40b0cc9b\",\n",
      "    \"text\": \"B All Experimental Results\",\n",
      "    \"metadata\": {\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"055f2fa97fbdee35766495a3452ebd9d\",\n",
      "    \"text\": \"This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs. chain-of-thought prompting.\",\n",
      "    \"metadata\": {\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"826446fa7830f0352c88808f40b0cc9b\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"NarrativeText\",\n",
      "    \"element_id\": \"9bf5af5255b80aace01b2da84ea86531\",\n",
      "    \"text\": \"For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly. A similar observation was made in Cobbe et al. (2021). Hence, we can further add a Python program as an external calculator (using the Python eval function) to all the equations in the generated chain of thought. When there are multiple equations in a chain of thought, we propagate the external calculator results from one equation to the following equations via string matching. As shown in Table 1, we see that adding a calculator signi\\ufb01cantly boosts performance of chain-of-thought prompting on most tasks.\",\n",
      "    \"metadata\": {\n",
      "      \"filetype\": \"application/pdf\",\n",
      "      \"languages\": [\n",
      "        \"eng\"\n",
      "      ],\n",
      "      \"page_number\": 1,\n",
      "      \"parent_id\": \"826446fa7830f0352c88808f40b0cc9b\",\n",
      "      \"filename\": \"CoT.pdf\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filename = \"./example_datasets/CoT.pdf\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    files=shared.Files(\n",
    "        content=f.read(), \n",
    "        file_name=filename,\n",
    "    )\n",
    "\n",
    "req = shared.PartitionParameters(\n",
    "    files=files,\n",
    "    strategy='hi_res',\n",
    "    pdf_infer_table_structure=True,\n",
    "    languages=[\"eng\"],\n",
    ")\n",
    "try:\n",
    "    resp = s.general.partition(req)\n",
    "    print(json.dumps(resp.elements[:3], indent=2))\n",
    "except SDKError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
