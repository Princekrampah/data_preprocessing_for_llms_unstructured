{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Image Analysis Techniques\n",
    "\n",
    "- Document layout detection\n",
    "- Vision transformers\n",
    "\n",
    "### Preprocessing Using Rule based parsers\n",
    "\n",
    "Some document types can be preprocessed using rule based techniques. Document types such as HTML, Markdown and Word Docs have predefined structures and rules that we can leverage to build a rule based parser to be used to preprocess them.\n",
    "\n",
    "#### Visual Information\n",
    "\n",
    "Some other documents can not be used with rule based preprocessors. Their layout information can only be understood visually. Example of such documents is PDf and Images.\n",
    "\n",
    "For Visual Information techniques, we have:\n",
    "\n",
    "1. **Document Image Analysis (DIA)**\n",
    "\n",
    "Allows us to extract the formatting information and text from the raw image of a document. Under this technique we have:\n",
    "\n",
    "#### a. Document layout detection (DLD)\n",
    "\n",
    "> This uses an object detection model to detect elements, label the elements and draw bounding boxes around each detected element of a document image and the text within each label gets extracted.\n",
    "\n",
    "Steps in document layout detection include:\n",
    "\n",
    "1. **Vision Detection**\n",
    "\n",
    "This involves identying and drawing bounding boxes around document elements using a computer vision model eg YOLOX or Detectron2.\n",
    "\n",
    "Reading sources:\n",
    "\n",
    "[OCR-free Document Understanding Transformer](https://arxiv.org/pdf/2111.15664)\n",
    "\n",
    "[YOLOX: Exceeding YOLO Series in 2021](https://arxiv.org/pdf/2107.08430)\n",
    "\n",
    "2. **Text Extraction**\n",
    "\n",
    "Extract the text from the detected bounding boxes. This is done using Object Character Recognition or OCR. For some documents, text can be extracted directly without the use of OCR. This is called **Direct Text extraction. Example in PDFs.\n",
    "\n",
    "#### b. Vision transformers (ViT)\n",
    "\n",
    "\n",
    "> These models take in document image as input and produce text representation of a structured output (Like JSON) as an output.\n",
    "\n",
    "\n",
    "A Vision Transformer (ViT) is a type of neural network model used for processing and understanding images. Here's a simple explanation:\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformers are a type of model originally developed for natural language processing (NLP) tasks, like translating languages or summarizing text. They are good at handling sequential data by focusing on the relationships between elements in the sequence using a mechanism called attention.\n",
    "\n",
    "### Applying Transformers to Images\n",
    "\n",
    "To apply transformers to images, we need to make a few adjustments, since images are not naturally sequential like text. Here's how it works:\n",
    "\n",
    "1. **Splitting the Image into Patches:**\n",
    "   - Instead of looking at the whole image at once, the Vision Transformer divides the image into smaller, fixed-size patches (like splitting an image into a grid of smaller squares).\n",
    "   - For example, a 256x256 image might be split into 16x16 patches.\n",
    "\n",
    "2. **Flattening the Patches:**\n",
    "   - Each patch is flattened into a single vector (a long row of numbers) representing the pixel values in that patch.\n",
    "\n",
    "3. **Embedding the Patches:**\n",
    "   - These vectors are then converted into a format the transformer can understand by using a process called embedding. This step is similar to how words are turned into word embeddings in NLP tasks.\n",
    "\n",
    "4. **Adding Positional Information:**\n",
    "   - Since the position of each patch in the image matters, we add positional encodings to the embeddings to give the model information about where each patch is located in the original image.\n",
    "\n",
    "5. **Processing with the Transformer:**\n",
    "   - The patches, now embedded and encoded with positional information, are fed into the transformer model.\n",
    "   - The transformer uses its attention mechanism to understand the relationships between patches and to process the image. It essentially looks at how different parts of the image relate to each other.\n",
    "\n",
    "6. **Classifying or Understanding the Image:**\n",
    "   - After processing the patches, the model can perform tasks like classifying what is in the image (e.g., identifying a cat, dog, or car) or other image-related tasks.\n",
    "\n",
    "### Advantages\n",
    "- **Flexibility:** Vision Transformers can handle different image sizes and resolutions.\n",
    "- **Scalability:** They can be scaled up easily, often resulting in better performance with more data and computational power.\n",
    "- **Performance:** They have shown excellent performance on various image recognition tasks, sometimes outperforming traditional convolutional neural networks (CNNs).\n",
    "\n",
    "### Summary\n",
    "In simple terms, a Vision Transformer takes an image, splits it into small pieces, and then processes these pieces using a transformer model to understand and analyze the image. It's a way of bringing powerful techniques from language processing to the world of computer vision.\n",
    "\n",
    "\n",
    "### Reading Sources:\n",
    "\n",
    "[DONUT Architecture](https://medium.com/@renix_informatics/how-to-use-donut-the-document-understanding-transformer-for-document-classification-parsing-and-fde0c7efa3f3)\n",
    "\n",
    "\n",
    "#### When To Use What?\n",
    "\n",
    "\n",
    "![DLD And ViT](./images/DLD_and_ViT.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20191125'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfminer\n",
    "\n",
    "pdfminer.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured_client import UnstructuredClient\n",
    "from unstructured_client.models import shared\n",
    "from unstructured_client.models.errors import SDKError\n",
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "import os\n",
    "\n",
    "from unstructured.staging.base import dict_to_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = UnstructuredClient(\n",
    "    api_key_auth=os.getenv(\"UNSTRUCTURED_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Reading document from string ...\n",
      "INFO: Reading document ...\n"
     ]
    }
   ],
   "source": [
    "filename = \"./example_datasets/el_nino.html\"\n",
    "html_elements = partition_html(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TITLE: CNN\n",
      "UNCATEGORIZEDTEXT: 1/30/2024\n",
      "TITLE: A potent pair of atmospheric rivers will drench California as El Niño makes its first mark on winter\n",
      "TITLE: By Mary Gilbert, CNN Meteorologist\n",
      "UNCATEGORIZEDTEXT: Updated: \n",
      "        3:49 PM EST, Tue January 30, 2024\n",
      "TITLE: Source: CNN\n",
      "NARRATIVETEXT: A potent pair of atmospheric river-fueled storms are about to unleash a windy and incredibly wet week in California in what is the first clear sign of the influence El Niño was expected to have on the state this winter.\n",
      "NARRATIVETEXT: The soaking storms will raise the flood threat across much of California into next week, but it appears the wet pattern is likely to continue well into February as a more typical El Niño pattern kicks into gear.\n",
      "NARRATIVETEXT: El Niño – a natural phenomenon in the tropical Pacific that influences weather around the globe – causes changes in the jet stream that can point storms directly at California. Storms can also tap into an extra-potent supply of moisture from the tropics called an atmospheric river.\n",
      "NARRATIVETEXT: El Niño hasn’t materialized many atmospheric rivers for California so far this winter, with most hitting the Pacific Northwest.\n"
     ]
    }
   ],
   "source": [
    "for element in html_elements[:10]:\n",
    "    print(f\"{element.category.upper()}: {element.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the Document with Document Layout Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./example_datasets/el_nino.pdf\"\n",
    "with open(filename, \"rb\") as f:\n",
    "    files=shared.Files(\n",
    "        content=f.read(),\n",
    "        file_name=filename,\n",
    "    )\n",
    "\n",
    "req = shared.PartitionParameters(\n",
    "    files=files,\n",
    "    strategy=\"hi_res\",\n",
    "    hi_res_model_name=\"yolox\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = s.general.partition(req)\n",
    "    dld_elements = dict_to_elements(resp.elements)\n",
    "except SDKError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: 1/30/24, 5:11 PM\n",
      "HEADER: CNN 1/30/2024\n",
      "HEADER: Pineapple express: California to get drenched by back-to-back storms fueling a serious ﬂood threat | CNN\n",
      "TITLE: A potent pair of atmospheric rivers will drench California as El Niño makes its ﬁrst mark on winter\n",
      "NARRATIVETEXT: By Mary Gilbert, CNN Meteorologist\n",
      "NARRATIVETEXT: Updated: 3:49 PM EST, Tue January 30, 2024\n",
      "NARRATIVETEXT: Source: CNN\n",
      "NARRATIVETEXT: A potent pair of atmospheric river-fueled storms are about to unleash a windy and incredibly wet week in California in what is the ﬁrst clear sign of the inﬂuence El Niño was expected to have on the state this winter.\n",
      "NARRATIVETEXT: The soaking storms will raise the ﬂood threat across much of California into next week, but it appears the wet pattern is likely to continue well into February as a more typical El Niño pattern kicks into gear.\n",
      "NARRATIVETEXT: El Niño – a natural phenomenon in the tropical Paciﬁc that inﬂuences weather around the globe – causes changes in the jet stream that can point storms directly at California. Storms can also tap into an extra-potent supply of moisture from the tropics called an atmospheric river.\n"
     ]
    }
   ],
   "source": [
    "for element in dld_elements[:10]:\n",
    "    print(f\"{element.category.upper()}: {element.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NarrativeText', 23), ('Title', 6), ('UncategorizedText', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_categories = [el.category for el in html_elements]\n",
    "collections.Counter(html_categories).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dld_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NarrativeText', 28), ('Header', 6), ('Title', 4), ('Footer', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dld_categories = [el.category for el in dld_elements]\n",
    "collections.Counter(dld_categories).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
